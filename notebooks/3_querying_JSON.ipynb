{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Working with JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from gzip import GzipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with GzipFile('../data/hn_dump.json.gz', 'r') as fin:\n",
    "    data = json.loads(fin.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100471"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': '2012-04-03T21:25:57.000Z',\n",
       " 'title': '',\n",
       " 'url': '',\n",
       " 'author': 'crag',\n",
       " 'points': 15,\n",
       " 'story_text': None,\n",
       " 'comment_text': 'Let me add another database that\\'s \"underestimated\" (by mainstream corporate America): SQLite3.<p>SQLite is fast, small, portable, easy &#38; simple to maintain and backup, AND reliable. And unless you are running a high traffic site (or application) it could handle everything a small (even medium) business would need.<p>Why small companies get talked into running MSQL or Oracle or MySQL is beyond me. And even if (and that\\'s a big IF) they needed more \"power\", there\\'s Postgres.<p>PS: Sorry for hijacking this thread. I\\'m a big fan boy of both SQLite and Postgres.',\n",
       " 'num_comments': None,\n",
       " 'story_id': 3793973,\n",
       " 'story_title': 'Postgres 9.2 will feature linear read scalability up to 64 cores',\n",
       " 'story_url': 'http://rhaas.blogspot.com/2012/04/did-i-say-32-cores-how-about-64.html',\n",
       " 'parent_id': 3794160,\n",
       " 'created_at_i': 1333488357,\n",
       " 'relevancy_score': 3843,\n",
       " '_tags': ['comment', 'author_crag', 'story_3793973'],\n",
       " 'objectID': '3794896',\n",
       " '_highlightResult': {'title': {'value': '',\n",
       "   'matchLevel': 'none',\n",
       "   'matchedWords': []},\n",
       "  'url': {'value': '', 'matchLevel': 'none', 'matchedWords': []},\n",
       "  'author': {'value': 'crag', 'matchLevel': 'none', 'matchedWords': []},\n",
       "  'comment_text': {'value': 'Let me add another database that\\'s \"underestimated\" (by mainstream corporate America): <em>SQLite</em>3.<p><em>SQLite</em> is fast, small, portable, easy & simple to maintain and backup, AND reliable. And unless you are running a high traffic site (or application) it could handle everything a small (even medium) business would need.<p>Why small companies get talked into running MSQL or Oracle or MySQL is beyond me. And even if (and that\\'s a big IF) they needed more \"power\", there\\'s Postgres.<p>PS: Sorry for hijacking this thread. I\\'m a big fan boy of both <em>SQLite</em> and Postgres.',\n",
       "   'matchLevel': 'full',\n",
       "   'fullyHighlighted': False,\n",
       "   'matchedWords': ['sqlite']},\n",
       "  'story_title': {'value': 'Postgres 9.2 will feature linear read scalability up to 64 cores',\n",
       "   'matchLevel': 'none',\n",
       "   'matchedWords': []},\n",
       "  'story_url': {'value': 'http://rhaas.blogspot.com/2012/04/did-i-say-32-cores-how-about-64.html',\n",
       "   'matchLevel': 'none',\n",
       "   'matchedWords': []}}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data).set_index('objectID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 100471 entries, 4616844 to 2226207\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   created_at        100471 non-null  object \n",
      " 1   title             91960 non-null   object \n",
      " 2   url               86089 non-null   object \n",
      " 3   author            100471 non-null  object \n",
      " 4   points            99126 non-null   float64\n",
      " 5   story_text        36569 non-null   object \n",
      " 6   comment_text      21000 non-null   object \n",
      " 7   num_comments      79471 non-null   float64\n",
      " 8   story_id          21003 non-null   float64\n",
      " 9   story_title       20977 non-null   object \n",
      " 10  story_url         20532 non-null   object \n",
      " 11  parent_id         21000 non-null   float64\n",
      " 12  created_at_i      100471 non-null  int64  \n",
      " 13  relevancy_score   88902 non-null   float64\n",
      " 14  _tags             100471 non-null  object \n",
      " 15  _highlightResult  100471 non-null  object \n",
      "dtypes: float64(5), int64(1), object(10)\n",
      "memory usage: 13.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlite3 import connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PATH = '../sqlite-olt.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect(DB_PATH) as db:\n",
    "    db.execute(\"create table if not exists hn_items_raw(data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~Dumping~ Writing Schemaless Data to a Relational Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT_ITEMS=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect(DB_PATH) as db:\n",
    "        db.execute(\"DELETE FROM hn_items_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934 ms ± 18.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "for item in data[:COUNT_ITEMS]:\n",
    "    with connect(DB_PATH) as db:\n",
    "        db.execute(\"insert into hn_items_raw(data) values (?)\", (json.dumps(item),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearing the DB to re-run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect(DB_PATH) as db:\n",
    "        db.execute(\"DELETE FROM hn_items_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.3 ms ± 778 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with connect(DB_PATH) as db:\n",
    "    for item in data[:COUNT_ITEMS]:\n",
    "        db.execute(\"insert into hn_items_raw(data) values (?)\", (json.dumps(item),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect(DB_PATH) as db:\n",
    "        db.execute(\"DELETE FROM hn_items_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.6 ms ± 1.79 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with connect(DB_PATH) as db:\n",
    "    db.executemany(\"insert into hn_items_raw(data) values (?)\", \n",
    "                   [(json.dumps(item),) for item in data[:COUNT_ITEMS]]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect(DB_PATH) as db:\n",
    "        db.execute(\"DELETE FROM hn_items_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's usually smart to write data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "471\n"
     ]
    }
   ],
   "source": [
    "for chunk in make_chunks(data, 1000):\n",
    "    with connect(DB_PATH) as db:\n",
    "        db.executemany(\"insert into hn_items_raw(data) values (?)\", \n",
    "                       [(json.dumps(item),) for item in chunk]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's tabulate (~ *normalize*) the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "objectID\n",
    "created_at\n",
    "title\n",
    "url\n",
    "author\n",
    "points\n",
    "story_text\n",
    "comment_text\n",
    "comment_text_length\n",
    "num_comments\n",
    "story_id\n",
    "story_title\n",
    "story_url\n",
    "parent_id\n",
    "relevancy_score\n",
    "tags\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect(DB_PATH) as db:\n",
    "    db.execute(\"drop view if exists hn_items_fields\")\n",
    "    db.execute(\"\"\"\n",
    "        create view if not exists hn_items_fields as\n",
    "        select \n",
    "            json_extract(data, '$.created_at') as created_at,\n",
    "            json_extract(data, '$.title') as title,\n",
    "            json_extract(data, '$.url') as url,\n",
    "            json_extract(data, '$.author') as author,\n",
    "            json_extract(data, '$.points') as points,\n",
    "            json_extract(data, '$.comment_text') as comment_text,\n",
    "            length(json_extract(data, '$.comment_text')) as comment_text_length,\n",
    "            json_extract(data, '$.story_text') as story_text,\n",
    "            json_extract(data, '$.story_id') as story_id,\n",
    "            json_extract(data, '$.story_title') as story_title,\n",
    "            json_extract(data, '$.story_url') as story_url,\n",
    "            json_extract(data, '$.story_text') as story_text,\n",
    "            json_extract(data, '$.parent_id') as parent_id,\n",
    "            json_extract(data, '$.relevancy_score') as relevancy_score,\n",
    "            json_extract(data, '$._tags') as tags \n",
    "        from hn_items_raw\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see what we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>points</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>comment_text_length</th>\n",
       "      <th>story_text</th>\n",
       "      <th>story_id</th>\n",
       "      <th>story_title</th>\n",
       "      <th>story_url</th>\n",
       "      <th>story_text:1</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [created_at, title, url, author, points, comment_text, comment_text_length, story_text, story_id, story_title, story_url, story_text:1, parent_id, relevancy_score, tags]\n",
       "Index: []"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with connect(DB_PATH) as db:\n",
    "    hn_items_fields = pd.read_sql('select * from hn_items_fields', db)\n",
    "    \n",
    "\n",
    "hn_items_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>points</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>comment_text_length</th>\n",
       "      <th>story_text</th>\n",
       "      <th>story_id</th>\n",
       "      <th>story_title</th>\n",
       "      <th>story_url</th>\n",
       "      <th>story_text:1</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [created_at, title, url, author, points, comment_text, comment_text_length, story_text, story_id, story_title, story_url, story_text:1, parent_id, relevancy_score, tags]\n",
       "Index: []"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn_items_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's find the most frequent authors in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"\"\"\n",
    "select json_extract(data, '$.author') as author, count(*) as count_author_comments\n",
    "from hn_items_raw\n",
    "group by author\n",
    "order by count_author_comments desc\n",
    "\"\"\"\n",
    "\n",
    "with connect(DB_PATH) as db:\n",
    "    frequent_authors_1 = pd.read_sql(query_1, db)\n",
    "    \n",
    "frequent_authors_1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = \"\"\"\n",
    "select author, count(*) as count_author_comments\n",
    "from hn_items_fields\n",
    "group by author\n",
    "order by count_author_comments desc\n",
    "\"\"\"\n",
    "\n",
    "with connect(DB_PATH) as db:\n",
    "    frequent_authors_1 = pd.read_sql(query_1, db)\n",
    "    \n",
    "frequent_authors_1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_author_query = \"\"\"\n",
    "select json_extract(data, '$.author'), json_extract(data, '$.objectID')\n",
    "from hn_items_raw\n",
    "where json_extract(data, '$.author') = 'luu'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "with connect(DB_PATH) as db:\n",
    "    luu_df = pd.read_sql(filter_author_query, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we speed this up ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We would usually create an index, but indices are defined on **columns**; where's the column here?\n",
    "- There's none, we haven't persisted data on disk, hence SQLite doesn't know where to look for pre-computed results\n",
    "- Instead we want to \"cache\" the result for the computation `json_extract(data, '$.author')`. \n",
    "- This is called an expression\n",
    "- In these senarios we create an *index on expression*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index on expression format \n",
    "\n",
    "`CREATE INDEX idx_name on TABLE_NAME (<expression_here>)`\n",
    "\n",
    "In `<expression_here>` we usually copy-paste the predicate from the `WHERE` clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_author_idx_query = \"\"\"\n",
    "create index if not exists idx_author on hn_items_raw (json_extract(data, '$.author'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect(DB_PATH) as db:\n",
    "    db.execute(create_author_idx_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "with connect(DB_PATH) as db:\n",
    "    luu_df = pd.read_sql(filter_author_query, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indices on expressions come usually handy in time-oriented computations.\n",
    "\n",
    "Say we want to filter comments posted on Sundays.\n",
    "\n",
    "The query would look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunday_comments=\"\"\"\n",
    "select json_extract(data, '$.comment_text'), datetime(json_extract(data, '$.created_at'))\n",
    "from hn_items_raw\n",
    "where strftime('%w', datetime(json_extract(data, '$.created_at'))) = '0'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "with connect(DB_PATH) as db:\n",
    "    sunday_comments_df = pd.read_sql(sunday_comments, db)\n",
    "    \n",
    "sunday_comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicate expression here is a little bit more complex, but certainly optimizable.\n",
    "\n",
    "Let's create an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index_on_sunday_comments_query =\\\n",
    "\"\"\"\n",
    "create index if not exists \n",
    "idx_comments_on_sundays on \n",
    "hn_items_raw (strftime('%w', datetime(json_extract(data, '$.created_at'))))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect(DB_PATH) as db:\n",
    "    db.execute(create_index_on_sunday_comments_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "with connect(DB_PATH) as db:\n",
    "    sunday_comments_df = pd.read_sql(sunday_comments, db)\n",
    "    \n",
    "sunday_comments_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
